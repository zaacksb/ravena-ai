const path = require('path');
const fs = require('fs').promises;
const Logger = require('../utils/Logger');
const LLMService = require('../services/LLMService');
const ReturnMessage = require('../models/ReturnMessage');
const Command = require('../models/Command');
const Database = require('../utils/Database');


const logger = new Logger('ai-commands');

const llmService = new LLMService({});
const database = Database.getInstance();

async function aiCommand(bot, message, args, group) {
  const chatId = message.group || message.author;

  // Contexto e descriÃ§Ã£o do bot
  const ctxPath = path.join(database.databasePath, 'textos', 'llm_context.txt');
  let ctxContent = await fs.readFile(ctxPath, 'utf8');

  const fixedCommands = bot.eventHandler.commandHandler.fixedCommands.getAllCommands();
  const managementCommands = bot.eventHandler.commandHandler.management.getManagementCommands();

  let cmdSimpleList = "";
  let cmdGerenciaSimplesList = "";

  for(let cmd of fixedCommands){
    if(cmd.description && cmd.description.length > 0 && !cmd.description.toLowerCase().includes("alias") && !cmd.hidden){
      const usage = cmd.usage ? ` | Uso: ${cmd.usage}`: "";
    	cmdSimpleList += `- ${bot.prefix}${cmd.name}: ${cmd.description}${usage}\n`;
    }
  }
  for(let cmd in managementCommands){
    const desc = managementCommands[cmd].description;
    cmdGerenciaSimplesList += `- ${bot.prefix}g-${cmd}: ${desc}\n`;
  }

  const variaveisReturn = await bot.eventHandler.commandHandler.management.listVariables(bot, message, args, group);
  const variaveisList = variaveisReturn.content;

  ctxContent += `\n\n## Comandos que vocÃª pode processar:\n\n${cmdSimpleList}\n\nPara os comandos personalizados criados com g-addCmd, vocÃª pode usar variÃ¡veis:\n${variaveisList}\n\nEstes sÃ£o os comandos usados apenas por administradores para gerenciarem seus grupos: ${cmdGerenciaSimplesList}\n\nSempre que for informar uma variÃ¡vel em um comando, use {} para encapsular ela, como {titulo}, {pessoa}. Quando o comando de gerencia pedir mÃ­dia, o comando deve ser enviado na legenda da foto/vÃ­deo ou em resposta (reply) Ã  mensagem que contÃ©m midia. Lembre o usuÃ¡rio que com o comando !g-painel algumas configuraÃ§Ãµes do gerenciar sÃ£o mais fÃ¡ceis de fazer, como mensagem de boas vindas e canais da twitch/youtube`;
  
  let question = message.caption ?? message.content;
  const quotedMsg = await message.origin.getQuotedMessage();
  if(quotedMsg){
    // Tem mensagem marcada, junta o conteudo (menos que tenha vindo de reaÃ§Ã£o)
    if(!message.originReaction){
      const quotedText = quotedMsg.caption ?? quotedMsg.content ?? quotedMsg.body;

      if(quotedText.length > 10){
        question += `\n\n${quotedText}`;
      }
    }
  }

  const media = await getMediaFromMessage(message);
  if (!media && question.length < 5) {
    return new ReturnMessage({
      chatId: chatId,
      content: 'Por favor, forneÃ§a uma pergunta ou uma imagem com uma pergunta. Exemplo: !ai Qual Ã© a capital da FranÃ§a?',
      options: {
        quotedMessageId: message.origin.id._serialized,
        evoReply: message.origin
      }
    });
  }
  
  //logger.debug(`Comando ai com pergunta: ctx: ${ctxContent}, \n Prompt: '${question}'`);
  
  const completionOptions = {
    prompt: question,
    systemContext: ctxContent
  };

  if (media && media.data) {
    logger.debug(`[aiCommand] Comando AI com mÃ­dia detectada: ${media.mimetype}`);
    if(media.mimetype.includes("image")){

      if(completionOptions.prompt.length < 4){
        completionOptions.prompt = "Analise esta imagem e entregue um resumo detalhado"
      }

      completionOptions.provider = 'lmstudio';
      completionOptions.image = media.data;
      
      // Quando interpretar imagens, usar um contexto diferente
      const ctxPath = path.join(database.databasePath, 'textos', 'llm_context_images.txt');
      completionOptions.systemContext = await fs.readFile(ctxPath, 'utf8') || "VocÃª se chama ravenabot e deve inter esta imagem enviada no WhatsApp";
    } else {
      return new ReturnMessage({
        chatId: chatId,
        content: `Ainda nÃ£o processo este tipo de arquivo (${media.mimetype}) ðŸ˜Ÿ Consigo apenas analisar imagens!`,
        options: {
          quotedMessageId: message.origin.id._serialized,
          evoReply: message.origin
        }
      });
    }
  }


  // ObtÃ©m resposta da IA
  try {
    logger.debug('[aiCommand] Tentando obter resposta do LLM', completionOptions);
    const response = await llmService.getCompletion(completionOptions);
    
    logger.debug('[aiCommand] Resposta LLM obtida, processando variaveis', response);

    let processedResponse;
    try{
      processedResponse = await bot.eventHandler.commandHandler.variableProcessor.process(response, {message, group, command: false, options: {}, bot });
    } catch(e){
      processedResponse = response;
    }
    
    // Retorna a resposta da IA
    return new ReturnMessage({
      chatId: chatId,
      content: processedResponse,
      options: {
        quotedMessageId: message.origin.id._serialized,
        evoReply: message.origin
      }
    });
  } catch (error) {
    logger.error('[aiCommand] Erro ao obter completaÃ§Ã£o LLM:', error);
    return new ReturnMessage({
      chatId: chatId,
      content: 'Desculpe, encontrei um erro ao processar sua solicitaÃ§Ã£o.',
      options: {
        quotedMessageId: message.origin.id._serialized,
        evoReply: message.origin
      }
    });
  }
}

// Auxiliar para obter mÃ­dia da mensagem
function getMediaFromMessage(message) {
  return new Promise((resolve, reject) => {
    // Se a mensagem tem mÃ­dia direta
    if (message.type !== 'text') {
      resolve(message.content);
      return;
    }
    
    // Tenta obter mÃ­dia da mensagem citada
    message.origin.getQuotedMessage()
      .then(quotedMsg => {
        if (quotedMsg && quotedMsg.hasMedia) {
          return quotedMsg.downloadMedia();
        }
        resolve(null);
      })
      .then(media => {
        if (media) resolve(media);
      })
      .catch(error => {
        logger.error('Erro ao obter mÃ­dia da mensagem citada:', error);
        resolve(null);
      });
  });
}

const commands = [
  new Command({
    name: 'ai',
    description: 'Pergunte algo Ã  IA',
    category: "ia",
    group: "askia",
    reactions: {
      trigger: "ðŸ¤–",
      before: process.env.LOADING_EMOJI ?? "ðŸŒ€",
      after: "ðŸ¤–"
    },
    cooldown: 5,
    method: aiCommand
  }),
  new Command({
    name: 'ia',
    description: 'Alias para AI',
    category: "ia",
    group: "askia",
    reactions: {
      trigger: "ðŸ¤–",
      before: process.env.LOADING_EMOJI ?? "ðŸŒ€",
      after: "ðŸ¤–"
    },
    cooldown: 5,
    method: aiCommand
  }), 
  new Command({
    name: 'gpt',
    hidden: true,
    description: 'Alias para AI',
    category: "ia",
    group: "askia",
    reactions: {
      trigger: "ðŸ¤–",
      before: process.env.LOADING_EMOJI ?? "ðŸŒ€",
      after: "ðŸ¤–"
    },
    cooldown: 5,
    method: aiCommand
  }), 
  new Command({
    name: 'gemini',
    hidden: true,
    description: 'Alias para AI',
    category: "ia",
    group: "askia",
    reactions: {
      trigger: "ðŸ¤–",
      before: process.env.LOADING_EMOJI ?? "ðŸŒ€",
      after: "ðŸ¤–"
    },
    cooldown: 5,
    method: aiCommand
  })
];

module.exports = { commands, aiCommand };
